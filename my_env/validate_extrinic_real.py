#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
validate_extrinic_real.py  â€”â€”  è¿æ¥çœŸå®ç›¸æœºéªŒè¯Eye-to-Handå¤–å‚
ç”¨æ³•ï¼špython validate_extrinic_real.py
éœ€è¦å®‰è£…: pip install pyrealsense2 opencv-python numpy scipy
"""

import numpy as np
import cv2
from scipy.spatial.transform import Rotation as R
import pyrealsense2 as rs
import time
from typing import Dict, List, Tuple, Optional
import argparse

# ä½ çš„å¤–å‚æ•°æ®
CAMERA_PARAMS = {
    "218622277783": {   # Camera 1  
        "qw": 0.23186468050094158,
        "qx": -0.9594348501025816,
        "qy": 0.11126460595906879,
        "qz": -0.11551504579753447,
        "x": -0.32427694851559585,
        "y": -0.28084946597353533,
        "z": 0.41925774261130094,
    },
    "130322272869": {   # Camera 2
        "qw": 0.06665864612025105,
        "qx": -0.160152624284172,
        "qy": 0.9519265051134965,
        "qz": -0.25247512886364,
        "x": -0.26168610660054314,
        "y": 0.2712285181679946,
        "z": 0.4070830794034206,
    },
    "819612070593": {   # Camera 3
        "qw": 0.15619147328055644,
        "qx": 0.6925084506280336,
        "qy": 0.6959352798619778,
        "qz": 0.10821439703958144,
        "x": -0.39398859088913385,
        "y": 0.01688281794116203,
        "z": 0.7609657167075485,
    },
}

# ArUco å‚æ•°
ARUCO_DICT = cv2.aruco.DICT_ARUCO_ORIGINAL
ARUCO_SIZE = 0.1  # ArUcoæ ‡è®°å®é™…å°ºå¯¸ï¼ˆç±³ï¼‰ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹

class RealSenseCamera:
    """RealSenseç›¸æœºç®¡ç†ç±»"""
    def __init__(self, serial_number: str):
        self.serial_number = serial_number
        self.pipeline = rs.pipeline()
        self.config = rs.config()
        
        # é…ç½®ç›¸æœº
        self.config.enable_device(serial_number)
        self.config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        
        # å¯åŠ¨ç›¸æœº
        try:
            self.profile = self.pipeline.start(self.config)
            
            # è·å–ç›¸æœºå†…å‚
            color_stream = self.profile.get_stream(rs.stream.color)
            self.intrinsics = color_stream.as_video_stream_profile().get_intrinsics()
            
            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            self.camera_matrix = np.array([
                [self.intrinsics.fx, 0, self.intrinsics.ppx],
                [0, self.intrinsics.fy, self.intrinsics.ppy],
                [0, 0, 1]
            ], dtype=np.float32)
            
            self.dist_coeffs = np.array(self.intrinsics.coeffs, dtype=np.float32)
            
            print(f"ç›¸æœº {serial_number} åˆå§‹åŒ–æˆåŠŸ")
            print(f"  åˆ†è¾¨ç‡: {self.intrinsics.width}x{self.intrinsics.height}")
            print(f"  ç„¦è·: fx={self.intrinsics.fx:.1f}, fy={self.intrinsics.fy:.1f}")
            
        except Exception as e:
            print(f"ç›¸æœº {serial_number} åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_frame(self) -> Optional[np.ndarray]:
        """è·å–ä¸€å¸§å›¾åƒ"""
        try:
            frames = self.pipeline.wait_for_frames(timeout_ms=5000)  # å¢åŠ è¶…æ—¶æ—¶é—´
            color_frame = frames.get_color_frame()
            if color_frame:
                return np.asanyarray(color_frame.get_data())
        except Exception as e:
            # ä¸æ‰“å°æ¯æ¬¡è·å–å¤±è´¥çš„é”™è¯¯ï¼Œåªåœ¨debugæ¨¡å¼ä¸‹æ˜¾ç¤º
            pass
        return None
    
    def stop(self):
        """åœæ­¢ç›¸æœº"""
        self.pipeline.stop()

class CameraExtrinsicsValidator:
    def __init__(self):
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(ARUCO_DICT)
        self.aruco_params = cv2.aruco.DetectorParameters()
        self.detector = cv2.aruco.ArucoDetector(self.aruco_dict, self.aruco_params)
        
        # åˆå§‹åŒ–ç›¸æœº
        self.cameras = {}
        self.transform_matrices = {}
        
        print("æ­£åœ¨åˆå§‹åŒ–ç›¸æœº...")
        available_cameras = self._find_available_cameras()
        
        for serial in CAMERA_PARAMS.keys():
            if serial in available_cameras:
                try:
                    self.cameras[serial] = RealSenseCamera(serial)
                    self.transform_matrices[serial] = self._params_to_transform_matrix(CAMERA_PARAMS[serial])
                except Exception as e:
                    print(f"è·³è¿‡ç›¸æœº {serial}: {e}")
            else:
                print(f"æœªæ‰¾åˆ°ç›¸æœº {serial}")
        
        if not self.cameras:
            raise Exception("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„ç›¸æœº")
        
        print(f"æˆåŠŸåˆå§‹åŒ– {len(self.cameras)} å°ç›¸æœº")
    
    def _find_available_cameras(self) -> List[str]:
        """æŸ¥æ‰¾å¯ç”¨çš„RealSenseç›¸æœº"""
        ctx = rs.context()
        devices = ctx.query_devices()
        serials = []
        for dev in devices:
            serials.append(dev.get_info(rs.camera_info.serial_number))
        return serials
    
    def _params_to_transform_matrix(self, params: Dict) -> np.ndarray:
        """å°†å››å…ƒæ•°+å¹³ç§»å‚æ•°è½¬æ¢ä¸º4x4å˜æ¢çŸ©é˜µ"""
        quat = [params['qx'], params['qy'], params['qz'], params['qw']]
        rotation = R.from_quat(quat)
        rotation_matrix = rotation.as_matrix()
        
        transform = np.eye(4)
        transform[:3, :3] = rotation_matrix
        transform[:3, 3] = [params['x'], params['y'], params['z']]
        
        return transform
    
    def detect_aruco_pose(self, image: np.ndarray, camera: RealSenseCamera) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """æ£€æµ‹ArUcoæ ‡è®°å¹¶ä¼°è®¡å…¶åœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„ä½å§¿"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        corners, ids, _ = self.detector.detectMarkers(gray)
        
        if ids is not None and len(ids) > 0:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„æ ‡è®°
            # åœ¨æ–°ç‰ˆæœ¬OpenCVä¸­ï¼ŒestimatePoseSingleMarkersç§»åŠ¨åˆ°äº†cv2å‘½åç©ºé—´
            try:
                # å°è¯•æ–°ç‰ˆæœ¬API
                rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(
                    corners, ARUCO_SIZE, 
                    camera.camera_matrix, 
                    camera.dist_coeffs
                )
            except AttributeError:
                # å¦‚æœæ–°ç‰ˆAPIä¸å¯ç”¨ï¼Œå°è¯•æ—§ç‰ˆAPIä½ç½®
                try:
                    rvecs, tvecs, _ = cv2.estimatePoseSingleMarkers(
                        corners, ARUCO_SIZE, 
                        camera.camera_matrix, 
                        camera.dist_coeffs
                    )
                except AttributeError:
                    # æ‰‹åŠ¨è®¡ç®—å§¿æ€ (fallbackæ–¹æ³•)
                    return self._estimate_pose_manual(corners[0], camera)
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶åæ ‡è½´ï¼ˆå¯é€‰ï¼Œç”¨äºå¯è§†åŒ–ï¼‰
            cv2.aruco.drawDetectedMarkers(image, corners, ids)
            for i in range(len(rvecs)):
                cv2.drawFrameAxes(image, camera.camera_matrix, camera.dist_coeffs, 
                                rvecs[i], tvecs[i], 0.03)
            
            return rvecs[0][0], tvecs[0][0]
        return None
    
    def _estimate_pose_manual(self, corners: np.ndarray, camera: RealSenseCamera) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """æ‰‹åŠ¨ä¼°è®¡ArUcoå§¿æ€ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        # å®šä¹‰ArUcoæ ‡è®°çš„3Dç‚¹ï¼ˆæ ‡è®°ä¸­å¿ƒä¸ºåŸç‚¹ï¼‰
        half_size = ARUCO_SIZE / 2.0
        object_points = np.array([
            [-half_size, -half_size, 0],
            [half_size, -half_size, 0],
            [half_size, half_size, 0],
            [-half_size, half_size, 0]
        ], dtype=np.float32)
        
        # ä½¿ç”¨solvePnPä¼°è®¡å§¿æ€
        success, rvec, tvec = cv2.solvePnP(
            object_points, 
            corners.reshape(-1, 2), 
            camera.camera_matrix, 
            camera.dist_coeffs
        )
        
        if success:
            return rvec.flatten(), tvec.flatten()
        return None
    
    def transform_to_base_frame(self, tvec: np.ndarray, rvec: np.ndarray, camera_serial: str) -> np.ndarray:
        """å°†ç›¸æœºåæ ‡ç³»ä¸­çš„ä½å§¿è½¬æ¢åˆ°æœºå™¨äººåŸºåº§åæ ‡ç³»"""
        # å°†rvecè½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
        rotation_matrix, _ = cv2.Rodrigues(rvec)
        
        # æ„å»ºArUcoåœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„4x4å˜æ¢çŸ©é˜µ
        aruco_in_cam = np.eye(4)
        aruco_in_cam[:3, :3] = rotation_matrix
        aruco_in_cam[:3, 3] = tvec
        
        # è½¬æ¢åˆ°åŸºåº§åæ ‡ç³»
        cam_to_base = self.transform_matrices[camera_serial]
        aruco_in_base = cam_to_base @ aruco_in_cam
        
        return aruco_in_base[:3, 3]  # åªè¿”å›ä½ç½®
    
    def capture_and_validate(self, num_samples: int = 30, show_images: bool = False) -> Dict:
        """æ•è·å¤šå¸§å›¾åƒå¹¶éªŒè¯å¤–å‚"""
        results = {cam_id: [] for cam_id in self.cameras.keys()}
        
        print(f"å¼€å§‹é‡‡é›†æ•°æ®ï¼Œå°†é‡‡é›† {num_samples} å¸§")
        print("è¯·ç¡®ä¿ArUcoæ ‡è®°åœ¨æ‰€æœ‰ç›¸æœºè§†é‡ä¸­ï¼ŒæŒ‰ 'q' é”®æå‰é€€å‡º")
        
        for i in range(num_samples):
            print(f"\ré‡‡é›†è¿›åº¦: {i+1}/{num_samples}", end="", flush=True)
            
            all_images = {}
            valid_detections = 0
            
            # ä»æ‰€æœ‰ç›¸æœºè·å–å›¾åƒ
            for cam_id, camera in self.cameras.items():
                image = camera.get_frame()
                if image is not None:
                    all_images[cam_id] = image.copy()
                    
                    # æ£€æµ‹ArUco
                    pose_result = self.detect_aruco_pose(image, camera)
                    if pose_result is not None:
                        rvec, tvec = pose_result
                        base_position = self.transform_to_base_frame(tvec, rvec, cam_id)
                        results[cam_id].append(base_position)
                        valid_detections += 1
            
            # æ˜¾ç¤ºå›¾åƒï¼ˆå¯é€‰ï¼‰
            if show_images and all_images:
                combined_image = self._combine_images(all_images)
                cv2.imshow("Camera Views", combined_image)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\nç”¨æˆ·ä¸­æ–­é‡‡é›†")
                    break
            
            time.sleep(0.1)  # çŸ­æš‚å»¶æ—¶
        
        print("\né‡‡é›†å®Œæˆ")
        
        if show_images:
            cv2.destroyAllWindows()
        
        return results
    
    def _combine_images(self, images: Dict[str, np.ndarray]) -> np.ndarray:
        """å°†å¤šä¸ªç›¸æœºå›¾åƒåˆå¹¶åˆ°ä¸€ä¸ªçª—å£ä¸­æ˜¾ç¤º"""
        if not images:
            return np.zeros((480, 640, 3), dtype=np.uint8)
        
        image_list = list(images.values())
        if len(image_list) == 1:
            return image_list[0]
        elif len(image_list) == 2:
            return np.hstack(image_list)
        else:  # 3ä¸ªæˆ–æ›´å¤š
            # ç¬¬ä¸€è¡Œæ”¾ä¸¤ä¸ªï¼Œç¬¬äºŒè¡Œæ”¾å‰©ä½™çš„
            row1 = np.hstack(image_list[:2])
            if len(image_list) == 3:
                # ç¬¬ä¸‰ä¸ªå›¾åƒéœ€è¦è°ƒæ•´å°ºå¯¸æ¥åŒ¹é…
                img3_resized = cv2.resize(image_list[2], (row1.shape[1], image_list[2].shape[0]))
                return np.vstack([row1, img3_resized])
            else:
                row2 = np.hstack(image_list[2:4])
                return np.vstack([row1, row2])
    
    def analyze_results(self, results: Dict) -> None:
        """åˆ†æç»“æœï¼Œæ‰¾å‡ºæœ‰é—®é¢˜çš„ç›¸æœº"""
        print("\n" + "="*60)
        print("å¤–å‚éªŒè¯ç»“æœåˆ†æ")
        print("="*60)
        
        # è®¡ç®—æ¯ä¸ªç›¸æœºçš„ç»Ÿè®¡ä¿¡æ¯
        avg_positions = {}
        valid_cameras = []
        
        for cam_id, positions in results.items():
            if len(positions) > 0:
                positions_array = np.array(positions)
                avg_pos = np.mean(positions_array, axis=0)
                std_pos = np.std(positions_array, axis=0)
                avg_positions[cam_id] = avg_pos
                valid_cameras.append(cam_id)
                
                print(f"\nç›¸æœº {cam_id}:")
                print(f"  æœ‰æ•ˆæ£€æµ‹æ¬¡æ•°: {len(positions)}")
                print(f"  å¹³å‡ä½ç½® (x,y,z): ({avg_pos[0]:.4f}, {avg_pos[1]:.4f}, {avg_pos[2]:.4f}) ç±³")
                print(f"  ä½ç½®æ ‡å‡†å·®:      ({std_pos[0]:.4f}, {std_pos[1]:.4f}, {std_pos[2]:.4f}) ç±³")
                print(f"  ä½ç½®ç¨³å®šæ€§:      {np.linalg.norm(std_pos)*1000:.2f} æ¯«ç±³")
            else:
                print(f"\nç›¸æœº {cam_id}: æœªæ£€æµ‹åˆ°ArUcoæ ‡è®°")
        
        if len(valid_cameras) < 2:
            print("\nâŒ é”™è¯¯: æœ‰æ•ˆç›¸æœºæ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ¯”è¾ƒåˆ†æ")
            print("   è¯·æ£€æŸ¥:")
            print("   1. ArUcoæ ‡è®°æ˜¯å¦åœ¨ç›¸æœºè§†é‡ä¸­")
            print("   2. å…‰ç…§æ˜¯å¦å……è¶³")
            print("   3. ArUcoæ ‡è®°å°ºå¯¸è®¾ç½®æ˜¯å¦æ­£ç¡®")
            return
        
        # è®¡ç®—ç›¸æœºé—´çš„ä½ç½®å·®å¼‚
        print(f"\nç›¸æœºé—´ä½ç½®å·®å¼‚åˆ†æ:")
        print("-" * 40)
        
        distances = {}
        for i, cam1 in enumerate(valid_cameras):
            for j, cam2 in enumerate(valid_cameras[i+1:], i+1):
                diff = avg_positions[cam1] - avg_positions[cam2]
                distance = np.linalg.norm(diff)
                distances[(cam1, cam2)] = distance
                
                print(f"ç›¸æœº {cam1[:8]}... vs {cam2[:8]}...:")
                print(f"  ä½ç½®å·®å¼‚: ({diff[0]:.4f}, {diff[1]:.4f}, {diff[2]:.4f}) ç±³")
                print(f"  ç©ºé—´è·ç¦»: {distance:.4f} ç±³ ({distance*100:.2f} å˜ç±³)")
                print()
        
        # é—®é¢˜è¯Šæ–­
        print("é—®é¢˜è¯Šæ–­:")
        print("-" * 20)
        
        max_distance = max(distances.values()) if distances else 0
        
        if max_distance > 0.05:  # 5å˜ç±³é˜ˆå€¼
            suspicious_pair = [k for k, v in distances.items() if v == max_distance][0]
            print(f"âš ï¸  å‘ç°å¼‚å¸¸: ç›¸æœºå·®å¼‚è¿‡å¤§")
            print(f"   æœ€å¤§å·®å¼‚: {max_distance*100:.2f} å˜ç±³")
            print(f"   æ¶‰åŠç›¸æœº: {suspicious_pair[0][:8]}... å’Œ {suspicious_pair[1][:8]}...")
            
            # è¿›ä¸€æ­¥åˆ†æå“ªä¸ªç›¸æœºæ›´å¯èƒ½æœ‰é—®é¢˜
            if len(valid_cameras) >= 3:
                cam_avg_distances = {}
                for cam in valid_cameras:
                    distances_for_cam = []
                    for pair, dist in distances.items():
                        if cam in pair:
                            distances_for_cam.append(dist)
                    cam_avg_distances[cam] = np.mean(distances_for_cam)
                
                most_suspicious = max(cam_avg_distances, key=cam_avg_distances.get)
                print(f"   æœ€å¯ç–‘çš„ç›¸æœº: {most_suspicious[:8]}...")
                print(f"   è¯¥ç›¸æœºçš„å¹³å‡å·®å¼‚: {cam_avg_distances[most_suspicious]*100:.2f} å˜ç±³")
        elif max_distance > 0.02:  # 2å˜ç±³
            print(f"âš ï¸  æ³¨æ„: å­˜åœ¨ä¸­ç­‰ç¨‹åº¦å·®å¼‚")
            print(f"   æœ€å¤§å·®å¼‚: {max_distance*100:.2f} å˜ç±³")
            print(f"   åœ¨å¯æ¥å—èŒƒå›´å†…ï¼Œä½†å»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥")
        else:
            print("âœ… å¤–å‚éªŒè¯é€šè¿‡")
            print(f"   æœ€å¤§å·®å¼‚: {max_distance*100:.2f} å˜ç±³")
            print(f"   æ‰€æœ‰ç›¸æœºå¤–å‚è´¨é‡è‰¯å¥½")
        
        # ç»™å‡ºå»ºè®®
        print(f"\nå»ºè®®:")
        print("-" * 10)
        if max_distance > 0.05:
            print("â€¢ ğŸ”´ éœ€è¦é‡æ–°æ ‡å®šå¤–å‚ï¼Œç‰¹åˆ«æ˜¯å·®å¼‚æœ€å¤§çš„ç›¸æœº")
            print("â€¢ æ£€æŸ¥ç›¸æœºå®‰è£…æ˜¯å¦ç¨³å›ºï¼Œé¿å…æŒ¯åŠ¨æˆ–æ¾åŠ¨")
            print("â€¢ ç¡®è®¤ArUcoæ ‡è®°å°ºå¯¸è®¾ç½®æ˜¯å¦æ­£ç¡®")
            print("â€¢ æ£€æŸ¥æ ‡å®šæ—¶çš„æ•°æ®è´¨é‡")
        elif max_distance > 0.02:
            print("â€¢ ğŸŸ¡ å»ºè®®æ£€æŸ¥å¤–å‚æ ‡å®šè´¨é‡")
            print("â€¢ å¯ä»¥å°è¯•å¢åŠ æ ‡å®šæ•°æ®é‡")
            print("â€¢ å½“å‰ç²¾åº¦å¯ç”¨äºå¤§éƒ¨åˆ†åº”ç”¨")
        else:
            print("â€¢ ğŸŸ¢ å¤–å‚æ ‡å®šè´¨é‡ä¼˜ç§€")
            print("â€¢ å¯ä»¥æ”¾å¿ƒç”¨äºé«˜ç²¾åº¦åº”ç”¨")
            print("â€¢ å»ºè®®å®šæœŸè¿›è¡ŒéªŒè¯")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        for camera in self.cameras.values():
            camera.stop()
        print("ç›¸æœºèµ„æºå·²é‡Šæ”¾")

def main():
    parser = argparse.ArgumentParser(description="Eye-to-Handç›¸æœºå¤–å‚éªŒè¯å·¥å…·")
    parser.add_argument("--samples", type=int, default=30, help="é‡‡é›†æ ·æœ¬æ•°é‡ (é»˜è®¤: 30)")
    parser.add_argument("--show", action="store_true", help="æ˜¾ç¤ºç›¸æœºç”»é¢")
    parser.add_argument("--aruco-size", type=float, default=0.05, help="ArUcoæ ‡è®°å°ºå¯¸(ç±³) (é»˜è®¤: 0.05)")
    
    args = parser.parse_args()
    
    global ARUCO_SIZE
    ARUCO_SIZE = args.aruco_size
    
    print("Eye-to-Hand ç›¸æœºå¤–å‚éªŒè¯å·¥å…·")
    print("=" * 50)
    print(f"ArUcoæ ‡è®°å°ºå¯¸: {ARUCO_SIZE} ç±³")
    print(f"é‡‡é›†æ ·æœ¬æ•°é‡: {args.samples}")
    print(f"æ˜¾ç¤ºç›¸æœºç”»é¢: {'æ˜¯' if args.show else 'å¦'}")
    print()
    
    try:
        validator = CameraExtrinsicsValidator()
        
        # æ˜¾ç¤ºå¤–å‚ä¿¡æ¯
        print("\nå½“å‰å¤–å‚é…ç½®:")
        for cam_id, params in CAMERA_PARAMS.items():
            if cam_id in validator.cameras:
                print(f"ç›¸æœº {cam_id[:8]}...:")
                print(f"  ä½ç½®: ({params['x']:.4f}, {params['y']:.4f}, {params['z']:.4f}) ç±³")
                quat = [params['qx'], params['qy'], params['qz'], params['qw']]
                euler = R.from_quat(quat).as_euler('xyz', degrees=True)
                print(f"  æ—‹è½¬: ({euler[0]:.1f}Â°, {euler[1]:.1f}Â°, {euler[2]:.1f}Â°)")
        print()
        
        # å¼€å§‹éªŒè¯
        results = validator.capture_and_validate(num_samples=args.samples, show_images=args.show)
        validator.analyze_results(results)
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
    finally:
        if 'validator' in locals():
            validator.cleanup()

if __name__ == "__main__":
    main() 